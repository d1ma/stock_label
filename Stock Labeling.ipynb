{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do standard imports for Python Data Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the dependency parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from main import Main\n",
    "from simplejson import loads\n",
    "m = Main()\n",
    "from dependency import StanfordParser\n",
    "sp = StanfordParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'root', u'ROOT', u'like'],\n",
       " [u'nsubj', u'like', u'I'],\n",
       " [u'dobj', u'like', u'roses'],\n",
       " [u'nsubj', u'red', u'that'],\n",
       " [u'cop', u'red', u'are'],\n",
       " [u'rcmod', u'roses', u'red'],\n",
       " [u'conj_and', u'roses', u'roses'],\n",
       " [u'nsubj', u'white', u'that'],\n",
       " [u'cop', u'white', u'are'],\n",
       " [u'rcmod', u'roses', u'white'],\n",
       " [u'root', u'ROOT', u'cry'],\n",
       " [u'nsubj', u'cry', u'Tonight'],\n",
       " [u'dep', u'Tonight', u'we'],\n",
       " [u'aux', u'cry', u'shall'],\n",
       " [u'poss', u'pillows', u'our'],\n",
       " [u'prep_in', u'cry', u'pillows']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse import stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['STANFORD_PARSER'] = os.path.abspath(\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1.jar\")\n",
    "os.environ['STANFORD_MODELS'] = os.path.abspath(\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1-models.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser.jar, see:\n    <http://nlp.stanford.edu/software/lex-parser.shtml>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c0ae94a49d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanford\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dima/Envs/vc/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STANFORD_PARSER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dima/Envs/vc/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    644\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    645\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 646\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdoutdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dima/Envs/vc/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    639\u001b[0m                     (name_pattern, url))\n\u001b[1;32m    640\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser.jar, see:\n    <http://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "parser = stanford.StanfordParser(model_path=\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RPCTransportError",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRPCTransportError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1f879370f737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dima/Documents/205/vc/dependency.py\u001b[0m in \u001b[0;36mget_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dima/Documents/205/vc/stanfordnlp/jsonrpc.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__req\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__req\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;31m#=========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dima/Documents/205/vc/stanfordnlp/jsonrpc.pyc\u001b[0m in \u001b[0;36m__req\u001b[0;34m(self, methodname, args, kwargs, id)\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0mresp_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendrecv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreq_str\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRPCTransportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_serializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads_response\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mresp_str\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRPCTransportError\u001b[0m: timed out"
     ]
    }
   ],
   "source": [
    "f = m.tfiles[5]\n",
    "sample_num = f.numbers[0]\n",
    "context = \"\".join(sample_num.context)\n",
    "r = sp.get_sentences(context)\n",
    "print len(sp.get_sentences(context))\n",
    "print dir(sp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Number Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tagged_file import *\n",
    "from util import *\n",
    "tfiles = read_directory(full_input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from featurize import *\n",
    "import random\n",
    "constr = FeatureConstructor(tfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now build a model based on these vectors\n",
    "num_files = len(tfiles)\n",
    "\n",
    "fraction_training = 3.0/4.0\n",
    "\n",
    "num_training_files = int(fraction_training * num_files)\n",
    "num_test_files = num_files - num_training_files\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(tfiles)\n",
    "train_files = tfiles[:num_training_files]\n",
    "test_files = tfiles[num_training_files:]\n",
    "\n",
    "# get all data for training\n",
    "all_data = [constr.get_feature_matrix_and_output_vector(f) for f in tfiles]\n",
    "train_data = [constr.get_feature_matrix_and_output_vector(f) for f in train_files]\n",
    "test_data = [constr.get_feature_matrix_and_output_vector(f) for f in test_files]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = np.vstack(d[0] for d in all_data)\n",
    "all_labels = np.hstack(d[1] for d in all_data)\n",
    "\n",
    "train_features = np.vstack([d[0] for d in train_data])\n",
    "train_label = np.hstack([d[1] for d in train_data])\n",
    "\n",
    "\n",
    "test_features = np.vstack([d[0] for d in test_data])\n",
    "test_label = np.hstack(d[1] for d in test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "Train score 0.932270916335\n",
      "Test score 0.877192982456\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.97      0.96       493\n",
      "        1.0       0.67      0.79      0.72        38\n",
      "        2.0       0.75      0.75      0.75        51\n",
      "        3.0       0.63      0.35      0.45        34\n",
      "\n",
      "avg / total       0.90      0.90      0.90       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create and train the SVM model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "clf = svm.SVC().fit(train_features, train_label)\n",
    "print clf\n",
    "predicted = cross_validation.cross_val_predict(clf, all_features, all_labels, cv=5 )\n",
    "\n",
    "train_score = clf.score(train_features, train_label)\n",
    "test_score = clf.score(test_features, test_label)\n",
    "print \"Train score\", train_score\n",
    "print \"Test score\", test_score\n",
    "print(classification_report(all_labels, predicted))\n",
    "\n",
    "\n",
    "# confidences = clf.decision_function(test_features)\n",
    "# print zip(confidences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      "Train score 0.982071713147\n",
      "Test score 0.842105263158\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.94      0.95       493\n",
      "        1.0       0.57      0.66      0.61        38\n",
      "        2.0       0.67      0.82      0.74        51\n",
      "        3.0       0.52      0.44      0.48        34\n",
      "\n",
      "avg / total       0.89      0.88      0.89       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create and train the decision trees model\n",
    "from sklearn import tree\n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "clf = tree.DecisionTreeClassifier().fit(train_features, train_label)\n",
    "print clf\n",
    "predicted = cross_validation.cross_val_predict(clf, all_features, all_labels, cv=4 )\n",
    "\n",
    "train_score = clf.score(train_features, train_label)\n",
    "test_score = clf.score(test_features, test_label)\n",
    "print \"Train score\", train_score\n",
    "print \"Test score\", test_score\n",
    "print(classification_report(all_labels, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Train score 0.982071713147\n",
      "Test score 0.929824561404\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.98      0.97       493\n",
      "        1.0       0.72      0.61      0.66        38\n",
      "        2.0       0.89      0.80      0.85        51\n",
      "        3.0       0.56      0.59      0.57        34\n",
      "\n",
      "avg / total       0.92      0.92      0.92       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create and train the random forest  model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100).fit(train_features, train_label)\n",
    "print clf\n",
    "predicted = cross_validation.cross_val_predict(clf, all_features, all_labels, cv=4 )\n",
    "\n",
    "train_score = clf.score(train_features, train_label)\n",
    "test_score = clf.score(test_features, test_label)\n",
    "print \"Train score\", train_score\n",
    "print \"Test score\", test_score\n",
    "print(classification_report(all_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'coref', u'sentences']\n",
      "[[u'root', u'ROOT', u'beautiful'], [u'nsubj', u'beautiful', u'It'], [u'cop', u'beautiful', u'is'], [u'advmod', u'beautiful', u'so']]\n"
     ]
    }
   ],
   "source": [
    "from stanfordnlp import jsonrpc\n",
    "from simplejson import loads\n",
    "server = jsonrpc.ServerProxy(jsonrpc.JsonRpc20(),\n",
    "                             jsonrpc.TransportTcpIp(addr=(\"localhost\", 8091)))\n",
    "\n",
    "result = loads(server.parse(\"Hello world.  It is so beautiful\"))\n",
    "#print \"Result\", result\n",
    "print result.keys()\n",
    "print result.get(\"sentences\")[1].get(\"dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.857512953368\n",
      "Test score 0.817391304348\n"
     ]
    }
   ],
   "source": [
    "# create and train the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0],\n",
      "       [1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 2],\n",
      "       [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 3],\n",
      "       [0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 4],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 5],\n",
      "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 9],\n",
      "       [1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 9],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 9],\n",
      "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 6],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 7],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 7]]), array([1, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0]))\n",
      "[13,500,000 at 1304 (tagged), 8,312,785 at 1390 (tagged), 436,507 at 1484 (tagged), 2,541,152 at 1599 (tagged), 2,824,209 at 1794 (tagged), 2,510,917 at 1908 (tagged)]\n",
      "['Common Shares', 'Preferred Shares', 'Series A1 Preferred Shares', 'Series A2 Preferred Shares', 'Series B Preferred Shares', 'Series C Preferred Shares']\n",
      "[u'B: common', u'B: common shares', u'B: common stock', u'B: number', u'B: number of', u'B: of', u'B: of shares', u'B: preferred', u'B: preferred shares', u'B: preferred stock', u'B: shares', u'B: stock', u'B: stock preferred', u'A: common', u'A: common shares', u'A: common stock', u'A: number', u'A: number of', u'A: of', u'A: of shares', u'A: preferred', u'A: preferred shares', u'A: preferred stock', u'A: shares', u'A: stock', u'A: stock preferred', u'Number Order']\n",
      "<FeatureConstructor> with 75 files, 3 features, 27 total columns\n"
     ]
    }
   ],
   "source": [
    "# Make sure featurizer works as expected\n",
    "sample_num = tfiles[2].numbers[0]\n",
    "print constr.get_feature_matrix_and_output_vector(tfiles[2])\n",
    "print tfiles[1].numbers\n",
    "labels =[num.label for num in tfiles[1].numbers]\n",
    "print [label.tag_key for label in labels]\n",
    "print constr.get_feature_names()\n",
    "print constr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all labels from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_files = os.listdir(full_input_dir)\n",
    "# full_path = [os.path.join(full_input_dir, x) for x in all_files ]\n",
    "# files = [x for x in full_path if os.path.isfile(x)]\n",
    "# regex = \"\\[\\[(.*?)\\]\\]\"\n",
    "# labels = []\n",
    "# match_objs = []\n",
    "# for f in files:\n",
    "#     with open(f, 'r') as in_f:\n",
    "#         for m in re.finditer(regex, in_f.read()):\n",
    "#             current_tag = m.group()\n",
    "#             labels += [current_tag]\n",
    "#             match_objs += [m]\n",
    "            \n",
    "# stripped_labels = [x[2:-2] for x in labels]\n",
    "# share_tags = [x.split(\":\")[0] for x in stripped_labels]\n",
    "# count_tags = [x.split(\":\")[1] for x in stripped_labels]\n",
    "# tags_to_count = {}\n",
    "# for label in stripped_labels:\n",
    "#     name, _ = label.split(\":\")\n",
    "#     count = tags_to_count.get(name, 0)\n",
    "#     tags_to_count[name] = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are some stats about the current labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== [Stats on the training data] =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-49c21ab7561a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"===== [Stats on the training data] =====\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Number of documents: %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Number of training samples: %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstripped_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Number of unique labels: %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print \"%s\" % \"\\n \".join([str(x) for x in sorted(tags_to_count.items(), key=lambda x: x[1])])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# print \"===== [Stats on the training data] =====\"\n",
    "# print \"Number of documents: %i\" % len(files)\n",
    "# print \"Number of training samples: %i\" % len(stripped_labels)\n",
    "# print \"Number of unique labels: %i\" % len(set(share_tags))\n",
    "# #print \"%s\" % \"\\n \".join([str(x) for x in sorted(tags_to_count.items(), key=lambda x: x[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is a number (we assume we can extract these reliably, an assumption to be tested), an context (sliding window, sentence, and sentence parse) from which we can come up with features to find a numerican representation for the number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' amended and restated in its entirety such that it reads as follows:\\r\\nFOURTH: The total number of shares of all classes of stock which the Corporation shall have authority to issue is (i) 13,500,000',\n",
       " ' shares of Common Stock, $0,001 par value per share (Common Stock), and (ii) 8,312,785 shares of')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num.context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of featurize failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"featurize.py\", line 109\n",
      "    return \"<FeatureConstructor> with %i files, %i features, %i total columns\" % ...\n",
      "                                                                                 ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# File exploration\n",
    "import re\n",
    "print \"Total files\", len(tfiles)\n",
    "test_words = ['total', 'share', 'shares', 'common shares', 'common stock', 'total number', 'total stock']\n",
    "# indx = {}\n",
    "# for word in test_words:\n",
    "#     indx[word] = []\n",
    "#     for t in tfiles:\n",
    "#         locs = [i for i in re.finditer(word, t.raw_clean.lower())]\n",
    "#         indx[word] += [locs]\n",
    "#     print \"Contain\", word, len([i for i in indx[word] if len(i) > 0])\n",
    "\n",
    "# more_than_1 = [i for (i, item) in enumerate(indx['total number']) if len(item) > 1]\n",
    "# print [t.context for t in tfiles[i].tags for i in more_than_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS TO MAKE OUTPUT LABELS FOR THE FIRST TASK\n",
    "\n",
    "_round1Labels = {\"TS\": ['Total Shares'], \"CS\": ['Common Shares'], \"PS\": [\"Preferred Shares\"]}\n",
    "labels = {}\n",
    "for key, val in _round1Labels.iteritems():\n",
    "    for v in val: \n",
    "        labels[v] = key\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 941 numbers\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS TO EXTRACT ALL NUMBERS AND THEIR LABELS AND OUTPUT THEM TO all_data.json\n",
    "all_data = []\n",
    "doc_index = 0\n",
    "for f in tfiles:\n",
    "    doc_id = doc_index\n",
    "    doc_index += 1\n",
    "    for num in f.numbers:\n",
    "        data = {}\n",
    "        \n",
    "        if num.label:\n",
    "            data['label'] = labels.get(num.label.tag_key, 'O')\n",
    "        else:\n",
    "            data['label'] = 'O'\n",
    "        data['before'] = num.context[0]\n",
    "        data['after'] = num.context[1]\n",
    "        data['doc_id'] = doc_id\n",
    "        all_data += [data]\n",
    "with open('all_data.json', 'w') as outp:\n",
    "    json.dump(all_data, outp)\n",
    "print \"Processed %i numbers\" % len(all_data)\n",
    "# print all_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join all contexts into a big list, forming the corpus\n",
    "corpus = []\n",
    "for t in tfiles:\n",
    "    for tag in t.tags:\n",
    "        corpus += [\"\".join(tag.context).lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct sparse matrix representing bag of words for the corpus\n",
    "sample_sentence = ['common shares preferred shares common stock preferred stock number of shares conversion']\n",
    "vectorizer = CountVectorizer(min_df = 1, binary=True, ngram_range=(1,2), vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z = N.zeros((403, 1))\n",
    "Xcpy = X.copy()\n",
    "\n",
    "res = N.append(z, Xcpy.toarray(), axis=1)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xapp = N.hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, u'conversion'),\n",
       " (True, u'shares common'),\n",
       " (True, u'shares conversion'),\n",
       " (True, u'shares preferred'),\n",
       " (True, u'stock number')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as N\n",
    "N.shape(Xarr)\n",
    "[(x,y) for (x,y) in zip((~Xarr.any(axis=0)),\n",
    "vectorizer.get_feature_names()) if x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File index 1\n",
      "Max number:  13,500,000 at 1304 (tagged)\n",
      "Tags []\n",
      "File index 2\n",
      "Max number:  34,125,000 at 2128 (tagged)\n",
      "Tags [Total Shares -> 34,125,000 @ 2128]\n",
      "File index 3\n",
      "Max number:  320,000,000 at 1213 (tagged)\n",
      "Tags []\n",
      "File index 4\n",
      "Max number:  60,950,000 at 13542 (tagged)\n",
      "Tags [Total Shares -> 60,950,000 @ 13542]\n",
      "File index 5\n",
      "Max number:  42,000,000 at 2143 (tagged)\n",
      "Tags []\n",
      "File index 6\n",
      "Max number:  11,248,385 at 1995 (tagged)\n",
      "Tags [Total Shares -> 11,248,385 @ 2003]\n",
      "File index 7\n",
      "Max number:  12,150,000 at 990 (tagged)\n",
      "Tags [Total Shares -> 12,150,000 @ 990]\n",
      "File index 8\n",
      "Max number:  22,830,769 at 1406 (tagged)\n",
      "Tags [Total Shares -> 22,830,769 @ 1406]\n",
      "File index 9\n",
      "Max number:  750099871,5 at 69809 (not tagged)\n",
      "Tags [Total Shares -> 84,560,775 @ 1816]\n",
      "File index 10\n",
      "Max number:  63,326,579 at 1753 (tagged)\n",
      "Tags [Total Shares -> 63,326,579 @ 1753]\n",
      "File index 11\n",
      "Max number:  14,125,849 at 1874 (tagged)\n",
      "Tags [Total Shares -> 14,125,849 @ 1874]\n",
      "File index 12\n",
      "Max number:  50,000,000 at 63941 (not tagged)\n",
      "Tags []\n",
      "File index 13\n",
      "Max number:  530,000,000 at 26499 (not tagged)\n",
      "Tags [Total Shares -> 23,250,000 @ 1113]\n",
      "File index 14\n",
      "Max number:  56,083,209 at 2288 (tagged)\n",
      "Tags [Total Shares -> 56,083,209 @ 2288]\n",
      "File index 15\n",
      "Max number:  20,000,000 at 1338 (tagged)\n",
      "Tags []\n",
      "File index 16\n",
      "Max number:  157,619,822 at 3112 (tagged)\n",
      "Tags []\n",
      "File index 17\n",
      "Max number:  65,000,000 at 7409 (tagged)\n",
      "Tags []\n",
      "File index 18\n",
      "Max number:  3903723,1 at 28756 (not tagged)\n",
      "Tags [Total Shares -> 16,000,000 @ 2035]\n",
      "File index 19\n",
      "Max number:  413625633,1 at 5358 (not tagged)\n",
      "Tags [Total Shares -> 114,535,766 @ 2309]\n",
      "File index 20\n",
      "Max number:  53,000,000 at 2850 (tagged)\n",
      "Tags []\n",
      "File index 21\n",
      "Max number:  50,000,000 at 13988 (not tagged)\n",
      "Tags [Total Shares -> 12,142,557 @ 2237]\n",
      "File index 22\n",
      "Max number:  100,000,000 at 28594 (not tagged)\n",
      "Tags [Total Shares -> 60,000,000 @ 5053]\n",
      "File index 23\n",
      "Max number:  10,000,000 at 787 (tagged)\n",
      "Tags [Total Shares -> 10,000,000 @ 787]\n",
      "File index 24\n",
      "Max number:  113,300,000 at 1572 (tagged)\n",
      "Tags [Total Shares -> 113,300,000 @ 1572]\n",
      "File index 25\n",
      "Max number:  100,000,000 at 57657 (not tagged)\n",
      "Tags [Total Shares -> 79.216.604 @ 1997]\n",
      "File index 26\n",
      "Max number:  110,000,000 at 1473 (tagged)\n",
      "Tags [Total Shares -> 110,000,000 @ 1473]\n",
      "File index 27\n",
      "Max number:  28,332,575 at 1852 (tagged)\n",
      "Tags []\n",
      "File index 28\n",
      "Max number:  125,296,900 at 1119 (tagged)\n",
      "Tags [Total Shares -> 125,296,900 @ 1119]\n",
      "File index 29\n",
      "Max number:  214,000,000 at 1063 (tagged)\n",
      "Tags []\n",
      "File index 30\n",
      "Max number:  50,056,880 at 3190 (tagged)\n",
      "Tags []\n",
      "File index 31\n",
      "Max number:  42,000,000 at 1307 (tagged)\n",
      "Tags []\n",
      "File index 32\n",
      "Max number:  126,343,303 at 852 (tagged)\n",
      "Tags [Total Shares -> 126,343,303 @ 852]\n",
      "File index 33\n",
      "Max number:  95,806,008 at 1060 (tagged)\n",
      "Tags [Total Shares -> 95,806,008 @ 1060]\n",
      "File index 34\n",
      "Max number:  10,000,000 at 821 (not tagged)\n",
      "Tags []\n",
      "File index 35\n",
      "Max number:  457,259,867 at 4583 (tagged)\n",
      "Tags [Total Shares -> 457,259,867 @ 4583]\n",
      "File index 36\n",
      "Max number:  43,061,725 at 1651 (tagged)\n",
      "Tags []\n",
      "File index 37\n",
      "Max number:  530,000,000 at 30001 (not tagged)\n",
      "Tags [Total Shares -> 14,000,000 @ 2176]\n",
      "File index 39\n",
      "Max number:  156,400,000 at 1740 (tagged)\n",
      "Tags [Total Shares -> 156,400,000 @ 1740]\n",
      "File index 40\n",
      "Max number:  30,000,000 at 875 (tagged)\n",
      "Tags []\n",
      "File index 41\n",
      "Max number:  150,000,000 at 19847 (not tagged)\n",
      "Tags [Total Shares -> 146,984,817 @ 1497]\n",
      "File index 42\n",
      "Max number:  110,000,000 at 1655 (tagged)\n",
      "Tags []\n",
      "File index 43\n",
      "Max number:  50,000,000 at 26148 (not tagged)\n",
      "Tags [Total Shares -> 155.887.572 @ 2102]\n",
      "File index 44\n",
      "Max number:  75,000,000 at 2191 (tagged)\n",
      "Tags [Total Shares -> 75,000,000 @ 2191]\n",
      "File index 45\n",
      "Max number:  58,832,033 at 1511 (tagged)\n",
      "Tags [Total Shares -> 58,832,033 @ 1511]\n",
      "File index 46\n",
      "Max number:  247,714,290 at 2537 (tagged)\n",
      "Tags [Total Shares -> 247,714,290 @ 2537]\n",
      "File index 47\n",
      "Max number:  45,000,000 at 31797 (not tagged)\n",
      "Tags [Total Shares -> 41,116,628 @ 1903]\n",
      "File index 48\n",
      "Max number:  581,382,356 at 2054 (tagged)\n",
      "Tags [Total Shares -> 581,382,356 @ 2054]\n",
      "File index 49\n",
      "Max number:  11,682,000 at 1542 (tagged)\n",
      "Tags [Total Shares -> 11,682,000 @ 1542]\n",
      "File index 50\n",
      "Max number:  330,000,000 at 1621 (tagged)\n",
      "Tags [Total Shares -> 625,480.000 @ 1570]\n",
      "File index 51\n",
      "Max number:  25,000,000 at 102559 (not tagged)\n",
      "Tags []\n",
      "File index 52\n",
      "Max number:  250,000,000 at 28883 (not tagged)\n",
      "Tags [Total Shares -> 179,634,796 @ 2198]\n",
      "File index 53\n",
      "Max number:  75,000,000 at 2599 (tagged)\n",
      "Tags []\n",
      "File index 54\n",
      "Max number:  4,03427791 at 19819 (not tagged)\n",
      "Tags [Total Shares -> 8,813,833 @ 1179]\n",
      "File index 55\n",
      "Max number:  50,000,000 at 16895 (not tagged)\n",
      "Tags [Total Shares -> 19.200,000 @ 1738]\n",
      "File index 56\n",
      "Max number:  70,000,000 at 2362 (tagged)\n",
      "Tags [Total Shares -> 70,000,000 @ 2362]\n",
      "File index 57\n",
      "Max number:  15,000,000 at 14650 (not tagged)\n",
      "Tags []\n",
      "File index 58\n",
      "Max number:  56,000,000 at 3166 (tagged)\n",
      "Tags []\n",
      "File index 59\n",
      "Max number:  65,595,0126 at 90018 (not tagged)\n",
      "Tags [Total Shares -> 65,452,222 @ 2371]\n",
      "File index 60\n",
      "Max number:  27,081,025 at 863 (tagged)\n",
      "Tags [Total Shares -> 27,081,025 @ 863]\n",
      "File index 61\n",
      "Max number:  70,000,000 at 1069 (tagged)\n",
      "Tags []\n",
      "File index 62\n",
      "Max number:  15,000,000 at 2638 (tagged)\n",
      "Tags [Total Shares -> 15,000,000 @ 2638]\n",
      "File index 63\n",
      "Max number:  550,000,000 at 59039 (not tagged)\n",
      "Tags [Total Shares -> 40,250,000 @ 1528]\n",
      "File index 64\n",
      "Max number:  100,000,000 at 53981 (not tagged)\n",
      "Tags [Total Shares -> 55,517,885 @ 1069]\n",
      "File index 65\n",
      "Max number:  702,224,500 at 1609 (tagged)\n",
      "Tags [Total Shares -> 702,224,500 @ 1609]\n",
      "File index 66\n",
      "Max number:  85,000,000 at 4565 (tagged)\n",
      "Tags [Total Shares -> 85,000,000 @ 4565]\n",
      "File index 67\n",
      "Max number:  154,092,906 at 2006 (tagged)\n",
      "Tags [Total Shares -> 154,092,906 @ 2006]\n",
      "File index 68\n",
      "Max number:  100,000,000 at 12258 (not tagged)\n",
      "Tags []\n",
      "File index 69\n",
      "Max number:  206,483,335 at 2505 (tagged)\n",
      "Tags []\n",
      "File index 70\n",
      "Max number:  389,704,272 at 2682 (tagged)\n",
      "Tags [Total Shares -> 389,704,272 @ 2682]\n",
      "File index 71\n",
      "Max number:  85,502,201 at 1516 (tagged)\n",
      "Tags [Total Shares -> 85,502,201 @ 1516]\n",
      "File index 72\n",
      "Max number:  129,518,884 at 2552 (tagged)\n",
      "Tags [Total Shares -> 129,518,884 @ 2552]\n",
      "File index 73\n",
      "Max number:  724,939,001 at 4745 (tagged)\n",
      "Tags []\n",
      "File index 74\n",
      "Max number:  152,129,232 at 993 (tagged)\n",
      "Tags [Total Shares -> 152,129,232 @ 1000]\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis: you can add the shares to find preferred and non-preferred :)\n",
    "\n",
    "for i,t in enumerate(tfiles):\n",
    "    if len(t.numbers) > 0:\n",
    "        print \"File index %i\" % i\n",
    "        print \"Max number: \", max(t.numbers, key=lambda x: x.num)\n",
    "        print \"Tags\", [tag for tag in t.tags if labels.get(tag.tag_key) == \"TS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'common', u'common shares', u'common stock', u'conversion', u'number', u'number of', u'of', u'of shares', u'preferred', u'preferred shares', u'preferred stock', u'shares', u'shares common', u'shares conversion', u'shares preferred', u'stock', u'stock number', u'stock preferred']\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_by_file = [t.tags for t in tfiles]\n",
    "all_tags = [item for sublist in tags_by_file for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tags = [tag for tag in all_tags if labels.get(tag, None) == 'TS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing:  Preferred Stock, $0,001 par value per share (Preferred Stock), of which (i) 436,507 shares have been designated as Series A-l Convertible Preferred Stock (Series A-l Preferred Stock), (ii) 2,541,152 shares have been designated as Series A-2 Convertible Preferred Stock (Series A-2 Preferred Stock\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'stanford'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-45e13863f878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Parsing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'stanford'"
     ]
    }
   ],
   "source": [
    "sample_sentence = all_tags[3].context\n",
    "print \"Parsing:\", sample_sentence\n",
    "result = nltk.parse.stanford(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Sentence Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each document by sentences, label each sentence as interesting or not (it's interesting if it has tags) and vectorize by three features. This will be used as training data for our decision tree for whether a sentence is interesting or not. \n",
    "\n",
    "Currently each sentence has three features that the decision tree will use. \n",
    "- Contains a number with commas\n",
    "- Sentence Length\n",
    "- Contains the word 'stock' or 'share'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10057 sentences\n",
      " 179 are interesting\n"
     ]
    }
   ],
   "source": [
    "from sentence import *\n",
    "sentence_objs = []\n",
    "features = [ContainsCommaNumber, SentenceLength, ContainsStockOrShare]\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as in_f:\n",
    "        raw_text = filter_ascii(in_f.read())\n",
    "        \n",
    "        # Create sentence objects: see sentence.py for the way this is done\n",
    "        for s in parser.tokenize(raw_text):\n",
    "            sentence_objs += [Sentence(s)]\n",
    "\n",
    "\n",
    "sentence_vecs = [s.to_vector(features) for s in sentence_objs]\n",
    "output_labels = [s.is_interesting_as_int() for s in sentence_objs]\n",
    "print \"Processed %i sentences\" % len(sentence_objs)\n",
    "print \" %i are interesting\" % output_labels.count(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== [Training report] ======\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      6619\n",
      "          1       0.94      0.92      0.93       119\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6738\n",
      "\n",
      "[[6612    7]\n",
      " [   9  110]]\n",
      "====== [Test report] =====\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      3259\n",
      "          1       0.76      0.78      0.77        60\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3319\n",
      "\n",
      "[[3244   15]\n",
      " [  13   47]]\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "from sklearn import tree, cross_validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "v_train, v_test, out_train, out_test, obj_train, obj_test = cross_validation.train_test_split(sentence_vecs, output_labels, sentence_objs, test_size=.33, random_state=0)\n",
    "clf = clf.fit(v_train, out_train)\n",
    "out_train_pred = clf.predict(v_train)\n",
    "out_test_pred = clf.predict(v_test)\n",
    "\n",
    "print \"====== [Training report] ======\"\n",
    "print classification_report(out_train, out_train_pred)\n",
    "cm_train = confusion_matrix(out_train, out_train_pred)\n",
    "# pl.matshow(cm_train)\n",
    "# pl.colorbar()\n",
    "# pl.show()\n",
    "print cm_train\n",
    "\n",
    "print \"====== [Test report] =====\"\n",
    "print classification_report(out_test, out_test_pred)\n",
    "cm_test = confusion_matrix(out_test, out_test_pred)\n",
    "# pl.matshow(cm_test)\n",
    "# pl.colorbar()\n",
    "# pl.show()\n",
    "print cm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-510bd847a9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtogether_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtogether_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfalse_neg_train_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtogether_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfalse_neg_test_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtogether_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "together_train = zip(out_train_pred, out_train)\n",
    "together_test = zip(out_test_pred, out_test)\n",
    "\n",
    "false_neg_train_i = [i for i,x  in enumerate(together_train) if (x[0] == 0 and x[1] == 1)]\n",
    "false_neg_test_i = [i for i, x in enumerate(together_test) if (x[0] == 0 and x[1] == 1)]\n",
    "\n",
    "false_neg_train = [str(obj_train[i]) for i in false_neg_train_i]\n",
    "false_neg_test = [str(obj_test[i]) for i in false_neg_test_i]\n",
    "\n",
    "print \"== Train ==\"\n",
    "print \"\\n-------\\n\".join(false_neg_train)\n",
    "\n",
    "print \"--------------------------------------------------\"\n",
    "print \"== Test ==\"\n",
    "print \"\\n-------\\n\".join(false_neg_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Interesting Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize each sentence, labelling it as interesting or not. Doing it using classification ('Interesting', 'Not') is a better long term approach because it allows for future labels to be added in addition to the stock data labelling. This might require new sentence level features to be added to `sentence.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Small Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching the labels within documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"The Corporation will have the authority to issue 14,000,000[[Total Shares:14,000,000]] shares of capital stock, $0.0001 par value per share, of which 10,000,000[[Common Shares:10,000,000]] shares will be Common Stock and of which 4,000,000[[Preferred Shares:4,000,000]] shares will be Preferred Stock.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59, 133, 184]\n",
      "The Corporation will have the authority to issue 14,000,000 shares of capital stock, $0.0001 par value per share, of which 10,000,000 shares will be Common Stock and of which 4,000,000 shares will be Preferred Stock.\n",
      "['14,000,000', '10,000,000', ' 4,000,000']\n"
     ]
    }
   ],
   "source": [
    "regex = \"\\[\\[(.*?)\\]\\]\"\n",
    "running_length = 0\n",
    "spans = []\n",
    "for m in re.finditer(regex, s):\n",
    "    sp = m.span()\n",
    "    adjusted_pos = sp[0] - running_length\n",
    "    spans += [adjusted_pos]\n",
    "    running_length += sp[1] - sp[0]\n",
    "\n",
    "print spans\n",
    "without_tags = re.sub(regex, \"\", s)\n",
    "print without_tags\n",
    "print [without_tags[(x-10): x] for x in spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[Preferred Shares:4,000,000]]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insertChars(s, c, *indeces)\n",
    "    \"\"\"Insert character `c` at `indeces` in the string `s`\"\"\"\n",
    "    splt = lambda s, i, : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc|de|f|ghijk'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tokenizing documents by sentences and matching integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "parser = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_paragraph = \"\"\"\n",
    "RTICLE IV\n",
    "Effective upon filing of this Amended and Restated Certificate of Incorporation with the Delaware Secretary of State, and prior to the issuance of any shares of Preferred Stock each currently outstanding share of Common Stock will be converted and reconstituted into 8.78204 shares of Common Stock of the Corporation (the “Stock Split”). No fractional shares or scrip representing fractional shares will be issued in connection with such Stock Split. If, after the aforementioned aggregation, the Stock Split would result in the issuance of any fractional share, the Corporation shall, in lieu of issuing any fractional share, pay cash equal to the product of such fraction multiplied by the Common Stock’s fair market value (as determined by the Board of Directors) on the date of conversion. All numbers of shares, and all amounts stated on a per share basis, contained in this Amended and Restated Certificate of Incorporation, are stated after giving effect to such Stock Split and no further adjustment shall be made as a consequence of such Stock Split.\n",
    "The Corporation will have the authority to issue 14,000,000[[Total Shares:14,000,000]] shares of capital stock, $0.0001 par value per share, of which 10,000,000[[Common Shares:10,000,000]] shares will be Common Stock and of which 4,000,000[[Preferred Shares:4,000,000]] shares will be Preferred Stock.\n",
    "11 7^1901? v5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "filter_ascii = lambda s: filter(lambda x: x in string.printable, s)\n",
    "clean_par = filter_ascii(sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = parser.tokenize(sample_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It's notable that the parser works pretty well. For instance, it does not split on the \".\" in \"$0.0001 per share\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RTICLE IV\n",
      "Effective upon filing of this Amended and Restated Certificate of Incorporation with the Delaware Secretary of State, and prior to the issuance of any shares of Preferred Stock each currently outstanding share of Common Stock will be converted and reconstituted into 8.78204 shares of Common Stock of the Corporation (the “Stock Split”).\n",
      " ------ \n",
      "No fractional shares or scrip representing fractional shares will be issued in connection with such Stock Split.\n",
      " ------ \n",
      "If, after the aforementioned aggregation, the Stock Split would result in the issuance of any fractional share, the Corporation shall, in lieu of issuing any fractional share, pay cash equal to the product of such fraction multiplied by the Common Stock’s fair market value (as determined by the Board of Directors) on the date of conversion.\n",
      " ------ \n",
      "All numbers of shares, and all amounts stated on a per share basis, contained in this Amended and Restated Certificate of Incorporation, are stated after giving effect to such Stock Split and no further adjustment shall be made as a consequence of such Stock Split.\n",
      " ------ \n",
      "The Corporation will have the authority to issue 14,000,000[[Total Shares:14,000,000]] shares of capital stock, $0.0001 par value per share, of which 10,000,000[[Common Shares:10,000,000]] shares will be Common Stock and of which 4,000,000[[Preferred Shares:4,000,000]] shares will be Preferred Stock.\n",
      " ------ \n",
      "11 7^1901?\n",
      " ------ \n",
      "v5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"\\n ------ \\n\".join(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUMPY Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 1)\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "flat_v = np.array([1, 2, 3])\n",
    "tall_v = np.array([[1], [2], [3]])\n",
    "reshape_v = tall_v.reshape(-1)\n",
    "print flat_v.shape\n",
    "print tall_v.shape\n",
    "print reshape_v.shape[0]\n",
    "print flat_v.reshape(-1).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14,000,000',\n",
       " '14,000,000',\n",
       " '10,000,000',\n",
       " '10,000,000',\n",
       " '4,000,000',\n",
       " '4,000,000']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_regex = r'\\d+(?:,\\d+)+'\n",
    "re.findall(integer_regex, sample_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrticle iv\\neffective upon filing of this amended and restated certificate of incorporation with the delaware secretary of state, and prior to the issuance of any shares of preferred stock each currently outstanding share of common stock will be converted and reconstituted into 8.78204 shares of common stock of the corporation (the \\xe2\\x80\\x9cstock split\\xe2\\x80\\x9d). no fractional shares or scrip representing fractional shares will be issued in connection with such stock split. if, after the aforementioned aggregation, the stock split would result in the issuance of any fractional share, the corporation shall, in lieu of issuing any fractional share, pay cash equal to the product of such fraction multiplied by the common stock\\xe2\\x80\\x99s fair market value (as determined by the board of directors) on the date of conversion. all numbers of shares, and all amounts stated on a per share basis, contained in this amended and restated certificate of incorporation, are stated after giving effect to such stock split and no further adjustment shall be made as a consequence of such stock split.\\nthe corporation will have the authority to issue 14,000,000[[total shares:14,000,000]] shares of capital stock, $0.0001 par value per share, of which 10,000,000[[common shares:10,000,000]] shares will be common stock and of which 4,000,000[[preferred shares:4,000,000]] shares will be preferred stock.\\n11 7^1901? v5\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_paragraph.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0, 53,  1]), array([ 0, 17,  1]), array([ 0, 56,  1]), array([ 0, 45,  1]), array([ 3, 35,  1]), array([0, 2, 0]), array([0, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "from sentence import * \n",
    "features = [ContainsCommaNumber, SentenceLength, ContainsStockOrShare]\n",
    "sentences = [filter_ascii(s) for s in sentences]\n",
    "sent_objs = [Sentence(s) for s in sentences]\n",
    "#print sent_objs\n",
    "#print [s.is_interesting() for s in sent_objs]\n",
    "print [s.to_vector(features) for s in sent_objs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
