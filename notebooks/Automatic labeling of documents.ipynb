{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# import everything from models: files, tags, etc\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 29), (6, 29), (12, 29), (14, 29), (9, 29), (4, 29), (6, 29), (1, 29), (7, 29), (18, 29), (9, 29), (20, 29), (8, 29), (19, 29), (11, 29), (3, 29), (11, 29), (17, 29), (6, 29), (13, 29), (8, 29), (5, 29), (11, 29), (1, 29), (6, 29), (8, 29), (3, 29), (2, 29), (12, 29), (11, 29), (12, 29), (2, 29), (11, 29), (8, 29), (1, 29), (9, 29), (10, 29), (9, 29), (0, 29), (10, 29), (5, 29), (16, 29), (8, 29), (10, 29), (14, 29), (17, 29), (10, 29), (11, 29), (12, 29), (4, 29), (11, 29), (21, 29), (31, 29), (11, 29), (13, 29), (6, 29), (12, 29), (5, 29), (8, 29), (30, 29), (6, 29), (11, 29), (3, 29), (29, 29), (27, 29), (12, 29), (32, 29), (9, 29), (9, 29), (12, 29), (12, 29), (18, 29), (12, 29), (22, 29), (16, 29)]\n"
     ]
    }
   ],
   "source": [
    "settings.set_up()\n",
    "r = main.Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCRed Document Structure -> Flat files + Company name JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Accruent LLC/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Arroweye Solutions, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Ascenta Therapeutics, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Azimuth Systems, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/BG Medicine, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/BiggerBoat/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Caden Biosciences Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Channel Intelligence, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/ClearCube Technology, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Digium, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Enfora, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/EnteroMedics, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Evolution Benefits, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/FibeRio Technology Corp/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Flowonix Medical, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Guild, The/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Handmark, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Highlightcam, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/HomeAway, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Hotel Booking Solutions, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Ice.com/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/iMo Corporation/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Incuity Software, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/InnerWireless, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Knoa Software, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/LifeMed Media, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/MariCal, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Mochila, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Music IP Corp/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/NEW EXPORT - txt/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/ObjectVideo, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Riot Games, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/SemiSouth Laboratories, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Serica Technologies, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/SiCortex, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Six Times Seven, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/TACODA, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Targanta Therapeutics, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Vertica Systems, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Voxant, Inc/Certificates of Incorporation doesn't have articles of incorporation\n",
      "!!! /Users/dima/Dropbox/Data Scrape of VC/OCRed Documents/Xactly Corporation/Certificates of Incorporation doesn't have articles of incorporation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from models.tagged_file import TFile\n",
    "import shutil\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "ocr_dir = os.path.join(home, \"Dropbox/Data Scrape of VC/OCRed Documents\")\n",
    "output_dir = os.path.join(home, \"Dropbox/Data Scrape of VC/txt_flat_files\")\n",
    "\n",
    "coi = \"Certificates of Incorporation\"\n",
    "unlabeled_tfiles = []\n",
    "name_to_company = {}\n",
    "i = 100\n",
    "for d in os.listdir(ocr_dir):\n",
    "    company_path = os.path.join(ocr_dir, d, coi)\n",
    "    if not os.path.isdir(company_path):\n",
    "        print \"!!! %s doesn't have articles of incorporation\" % company_path\n",
    "        continue\n",
    "        \n",
    "    company_name = d\n",
    "    for article in os.listdir(company_path):\n",
    "        article_path = os.path.join(company_path, article)\n",
    "        \n",
    "        if os.path.isfile(article_path):\n",
    "            unlabeled_tfiles += [TFile(article_path, i)]\n",
    "            i += 1\n",
    "            name_to_company[article_path] = company_name\n",
    "            shutil.copy(article_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled_tfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marathon Technologies 02112010.txt']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "nametail_to_company = {}\n",
    "for key, item in name_to_company.iteritems():\n",
    "    _, key_tail = os.path.split(key)\n",
    "    nametail_to_company[key_tail] = item\n",
    "print nametail_to_company.keys()[:1]\n",
    "    \n",
    "with open(\"name_to_company.json\",'w') as out_f:\n",
    "    json.dump(name_to_company, out_f)\n",
    "    \n",
    "with open(\"nametail_to_company.json\", 'w') as out_f:\n",
    "    json.dump(nametail_to_company, out_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dir = os.path.join(home, \"Dropbox/Data Scrape of VC/txt_flat_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_train_set = unlabeled_tfiles[:300]\n",
    "rest_of_set = unlabeled_tfiles[300:]\n",
    "\n",
    "\n",
    "file_set = small_train_set #unlabeled_tfiles, rest_of_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in file_set:\n",
    "    if len(f.numbers) > 0:\n",
    "        f.predictions = zip(r.cl.predict_class_for_each_number(f),f.numbers)\n",
    "    else:\n",
    "        f.predictions = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over the new files, see if they belong to previous. If not, then add them to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 3001)\n",
    "client.database_names()\n",
    "db = client.meteor\n",
    "from pymongo import collection\n",
    "from pymongo import InsertOne\n",
    "\n",
    "## Get the databases. Add create=True to create if needed\n",
    "docs = collection.Collection(db, 'docs')\n",
    "tags = collection.Collection(db, 'tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 3 in total\n",
      "Will write 297 new docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "requests = []\n",
    "with open(\"existing_docs.json\") as exist_file:\n",
    "    existing_files = set(json.load(exist_file))\n",
    "skipped = 0\n",
    "\n",
    "def generate_string_id(i=None):\n",
    "    if i:\n",
    "        return str(int(time.time() * 1000)) + str(i) + str(int(random.random() * 1000))\n",
    "    else:\n",
    "        return str(int(time.time() * 1000)) + str(int(random.random() * 10000))\n",
    "\n",
    "# for f in unlabeled_tfiles:\n",
    "# for f in rest_of_set:\n",
    "for f in file_set:\n",
    "    \n",
    "    doc_repr = {}\n",
    "    doc_repr['contents'] = f.raw_clean\n",
    "    doc_repr['filename'] = f.filename\n",
    "    head, tail = os.path.split(f.filename)\n",
    "    if tail in existing_files:\n",
    "        skipped += 1\n",
    "        docs.update({\"filename_tail\": tail}, {\"$set\": {\"company\": name_to_company[f.filename]}})\n",
    "        continue\n",
    "    \n",
    "    doc_repr['filename_tail'] = tail\n",
    "    doc_repr['old_id'] = f.id\n",
    "    doc_repr['batch'] = 2\n",
    "    doc_repr['company'] = name_to_company[f.filename]\n",
    "    doc_repr['tags'] = []\n",
    "    doc_repr['reviewed'] = False\n",
    "    i = 0\n",
    "    for prediction, number in f.predictions:\n",
    "        \n",
    "        update_doc = True\n",
    "        rand_forrest_prediction = prediction[1]\n",
    "        \n",
    "        translation = {'CS': \"Common Shares\", 'PS': \"Preferred Shares\", 'TS': 'Total Shares'}\n",
    "        description = translation.get(rand_forrest_prediction, None)\n",
    "        text = number.match\n",
    "        tag_repr = {'pos_start': number.pos - len(text), 'pos_end': number.pos, 'text': text,\n",
    "                   'needs_approval': False, 'document_id': doc['_id'], 'id': generate_string_id(i)}\n",
    "        if description:\n",
    "            tag_repr['description'] = description\n",
    "            tag_repr['needs_approval'] = True\n",
    "        \n",
    "        doc_repr['tags'].append(tag_repr)\n",
    "        i += 1\n",
    "    \n",
    "    requests.append(InsertOne(doc_repr))\n",
    "print \"Skipped %i in total\" %skipped\n",
    "print \"Will write %i new docs\" % len(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.BulkWriteResult at 0x1187163c0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add documents to the collection\n",
    "docs.bulk_write(requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Will insert 4725 new tags\n"
     ]
    }
   ],
   "source": [
    "tag_requests = []\n",
    "skipped = 0\n",
    "\n",
    "for i, f in enumerate(file_set):\n",
    "    if (i%1000 == 0):\n",
    "        print i\n",
    "    head, tail = os.path.split(f.filename)\n",
    "    if tail in existing_files:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    doc = docs.find_one({\"old_id\": f.id})\n",
    "    update_doc = False\n",
    "    for prediction, number in f.predictions:\n",
    "        update_doc = True\n",
    "        rand_forrest_prediction = prediction[1]\n",
    "        \n",
    "        translation = {'CS': \"Common Shares\", 'PS': \"Preferred Shares\", 'TS': 'Total Shares'}\n",
    "        description = translation.get(rand_forrest_prediction, None)\n",
    "        text = number.match\n",
    "        tag_repr = {'pos_start': number.pos - len(text), 'pos_end': number.pos, 'text': text, 'human_approved': False,\n",
    "                   'hide': False, 'human_created': False, 'document_id': doc['_id']}\n",
    "        if description:\n",
    "            tag_repr['description'] = description\n",
    "        \n",
    "        tag_requests.append(InsertOne(tag_repr))\n",
    "#     if update_doc:\n",
    "#         docs.update_one({\"old_id\": f.id}, {\"$set\": {\"\"}})\n",
    "print \"Will insert %i new tags\" % len(tag_requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.BulkWriteResult at 0x1186f6a00>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.bulk_write(tag_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5543\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "print tags.find({}).count()\n",
    "print docs.find({}).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
